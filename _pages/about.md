---
permalink: /
title: "About me"
excerpt: "Xiaoyu Li"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I am a Ph.D student in System Engineering at Boston University, supervised by Prof. [Francesco Orabona](http://francesco.orabona.com). I am a member of [OPTIMAL Lab](https://sites.google.com/view/optimal-lab/home). 

My research interests lie in theoretical machine learning and stochastic optimization. I currently work on understanding and designing optimization methods in machine learning, specifically, stochastic gradient descent and its variants, and adaptive gradient descent methods.   

I received my Bachelor degree in Math and Applied Math from [University of Science and Technology of China](http://en.ustc.edu.cn).  


Preprints
======
- <b>On the Last Iterate Convergence of Momentum Methods</b>.       
 <b>Xiaoyu Li</b>, Mingrui Liu, Francesco Orabona. arXiv preprints Feb. 2021. [Paper](https://arxiv.org/abs/2102.07002) 
 
 Publications
======
- <b>A Second look at Exponential and Cosine Step Sizes: Simplicity, Convergence, and Performance</b>.       
 <b>Xiaoyu Li</b>, Zhenxun Zhuang, Francesco Orabona. ICML 2021. [Paper](https://arxiv.org/abs/2002.05273) [Code](https://github.com/zhenxun-zhuang/SGD-Exponential-Stepsize)

-  <b>A High Probability Analysis of Adaptive SGD with Momentum</b>.                               
 <b>Xiaoyu Li</b>, Francesco Orabona. ICML 2020 Workshop on <i>Beyond First Order Methods in ML Systems</i>. [Paper](https://arxiv.org/abs/2007.14294) [Video](https://drive.google.com/file/d/1NlRfBisiuAcHjjEebiufpAyJUqE_kZTb/view)

- <b>On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes</b>.                               
 <b>Xiaoyu Li</b>, Francesco Orabona. AISTATS 2019. [Paper](http://proceedings.mlr.press/v89/li19c)

Experiences
======

Work
---- 
- Applied Scientist Intern @ Amazon, Remote.                             June - Aug. 2021
- Research Intern @ Nokia Bell Labs, Murray Hill, NJ.                    June - Aug. 2019  

Academic Service 
----
Reviewer of NeurIPS2019 & 2020; AISTATS2020-2022;  ICLR 2021, ICML2020 & 2021



